{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Functions for Preprocessing Earning Call Transcripts 1.0\n",
        "# Author: Yuchen Zhou\n",
        "# The script contains alll the utility functions for  preprocessing the texts from earning call transcript\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import contractions\n",
        "import spacy_transformers\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "OOtb9ISwd2RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Initial Preprocessing before Tokenization**"
      ],
      "metadata": {
        "id": "pnjtpcwumohv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 load_data"
      ],
      "metadata": {
        "id": "_61MqwCybWas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_data(file_name)\n",
        "\n",
        "This function will read the csv file that contains the earning call transcripts and store the data into a Pandas data frame.\n",
        "\n",
        "**Parameter**(file_name): the name of the csv file that contains data\n",
        "\n",
        "**Return**(raw_earning_call_transcript): the Pandas data frame that stores data\n"
      ],
      "metadata": {
        "id": "jmpbMgh0cALh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PEiFTbVaexn"
      },
      "outputs": [],
      "source": [
        "def load_data (file_name):\n",
        "  raw_earning_call_transcripts = pd.read_csv(file_name)\n",
        "  return raw_earning_call_transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 entity_removal"
      ],
      "metadata": {
        "id": "9piOGbFEiDCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "entity_removal(text,nlp)\n",
        "\n",
        "This function will take a text and remove entities that are not important for topic modeling (including person, date, moeny, percent, ordinal, cardinal, and time).\n",
        "\n",
        "**Parameter**(text): an imput string or text\n",
        "\n",
        "**Parameter**(nlp): Spacy's natural language processing model\n",
        "\n",
        "**Return**(newString): the new string after removal of unimportant entities"
      ],
      "metadata": {
        "id": "jH4kBzkciJB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_removal(text,nlp):\n",
        "  remove_entity_list = [\"PERSON\",\"DATE\",\"MONEY\",\"PERCENT\",\"ORDINAL\",\"CARDINAL\",\"TIME\"] # entities that will be removed\n",
        "  doc = nlp(text)\n",
        "  newString = text\n",
        "  for e in reversed(doc.ents):\n",
        "    if e.label_ in remove_entity_list:\n",
        "      newString = newString[:e.start_char] + newString[e.start_char + len(e.text):] # create a mew string without those entities\n",
        "  return newString"
      ],
      "metadata": {
        "id": "YIxS3bnhchfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 split_and_entity_removal"
      ],
      "metadata": {
        "id": "JwH3qkkMj7gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " split_and_entity_removal(my_dataframe)\n",
        "\n",
        "This function will split an earning transcrip into each response (each person's talking) and call functions to remove unimportant entities.\n",
        "\n",
        "**Parameter** (my_dataframe): Pandas data frame that contains all earning transcript data\n",
        "\n",
        "**Return** (earning_transcripts_name_removed): a list of earning transcripts that were splitted, and unimportant entities were removed from the transcripts"
      ],
      "metadata": {
        "id": "tffU04Sukwax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_and_entity_removal(my_dataframe):\n",
        "  nlp=spacy.load(\"en_core_web_trf\") # load Spacy's model\n",
        "  earning_transcripts =my_dataframe.content.values.tolist()\n",
        "  earning_transcripts_remove_operator=[]\n",
        "  for earning_transcript in earning_transcripts:\n",
        "     single_transcript=(re.sub('Operator:.*\\n', '', earning_transcript))\n",
        "     single_transcript=re.sub('Operator:.*\\Z', '', single_transcript)\n",
        "     earning_transcripts_remove_operator.extend(single_transcript.split(\":\")) # split earning transcripts into each response (each person's talking)\n",
        "\n",
        "  earning_transcripts_name_removed=[]\n",
        "  for earning_transcript in earning_transcripts_remove_operator:\n",
        "      earning_transcript=earning_transcript.replace(\"Ã¢\",\" \") # remove unkown characters\n",
        "      new_transcript=entity_removal(earning_transcript,nlp) # calling function to remove unimportant entities\n",
        "      new_transcript=new_transcript.replace(\"[Operator Instructions]\",\" \") # remove following words which were not important for topic modeling\n",
        "      new_transcript=new_transcript.replace(\"Operator\",\" \")\n",
        "      new_transcript=new_transcript.replace(\"operator\",\" \")\n",
        "      new_transcript=new_transcript.replace(\"Thank\",\" \")\n",
        "      new_transcript=new_transcript.replace(\"Thanks\",\" \")\n",
        "      new_transcript=new_transcript.replace(\"thank\",\" \")\n",
        "      new_transcript=new_transcript.replace(\"thanks\",\" \")\n",
        "\n",
        "      if len(new_transcript)<=10: # ignore strings which are short in length after entity removal and splitting\n",
        "        continue\n",
        "      else: # add the transcripts after entity removal and splitting into a new list\n",
        "        earning_transcripts_name_removed.append(new_transcript)\n",
        "  return earning_transcripts_name_removed"
      ],
      "metadata": {
        "id": "xE1FmoaGhXWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 split_and_expand_contraction"
      ],
      "metadata": {
        "id": "97v82Bxgm6ZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "split_and_expand_contraction(earning_transcripts)\n",
        "\n",
        "This function will expand the contractions in the earning transcripts and further split each response(each person's talking) into single sentences\n",
        "\n",
        "**Parameter** (earning_transcripts): the output from 1.3\n",
        "\n",
        "**Parameter** (earning_transcripts_single_sentence): a list of strings. Each string is a single sentence\n",
        "\n",
        "**Return** (earning_transcripts_single_sentence): a list of strings. Each string is a single sentence from earning transcripts"
      ],
      "metadata": {
        "id": "MZaLch-6nEkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_and_expand_contraction(earning_transcripts):\n",
        "  earning_transcripts_single_sentence=[]\n",
        "  for single_transcript in earning_transcripts:\n",
        "    single_transcript = contractions.fix(str(single_transcript)) # contractions were expanded\n",
        "    earning_transcripts_single_sentence.extend(single_transcript.split(\".\")) # strings were further splitted into single sentences\n",
        "  return earning_transcripts_single_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "eTSp64vgkUPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Tokenization and Further Preprocessing**"
      ],
      "metadata": {
        "id": "p6JCMKRdn3Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 lower_case_and_tokenize"
      ],
      "metadata": {
        "id": "g55Z8QU5oBd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lower_case_and_tokenize(earning_transcripts)\n",
        "\n",
        "This function will tokenize each sentence from earning transcript and remove punctuations\n",
        "\n",
        "**Parameter** (earning_transcripts): the output from 1.4\n",
        "\n",
        "**Return** (tokenized_earning_transcripts): A list of sentences which are tokenized (a list of lists of words)"
      ],
      "metadata": {
        "id": "u-cD3ezfoH2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_case_and_tokenize(earning_transcripts):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+') # set a tokenizer that filters out punctuations\n",
        "  tokenized_earning_transcripts=[]\n",
        "  for single_transcript in earning_transcripts:\n",
        "    single_transcript = single_transcript.lower() # make each sentence into lower case\n",
        "    single_transcript = tokenizer.tokenize(single_transcript) # tokenize and append into a new list\n",
        "    tokenized_earning_transcripts.append(single_transcript)\n",
        "  return tokenized_earning_transcripts"
      ],
      "metadata": {
        "id": "9mIbtzoON1Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 bigram_trigram"
      ],
      "metadata": {
        "id": "zGeSOsM6pY59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bigram_trigram(earning_transcripts)\n",
        "\n",
        "This function will generate bigram and trigram from tokenized sentences\n",
        "\n",
        "**Parameter** (earning_transcripts): tokenized sentences (output from 2.1)\n",
        "\n",
        "**Return** (earning_transcripts_trigram): a list of tokenized sentences with bigram and trigram"
      ],
      "metadata": {
        "id": "Ypca1EEdpe6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_trigram(earning_transcripts):\n",
        "  bigram = gensim.models.Phrases(earning_transcripts, min_count=5, threshold=50) # generate bigram\n",
        "  trigram = gensim.models.Phrases(bigram[earning_transcripts], threshold=30) # generate trigram\n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "  trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "  earning_transcripts_trigram=[trigram_mod[bigram_mod[doc]] for doc in earning_transcripts]\n",
        "  return earning_transcripts_trigram"
      ],
      "metadata": {
        "id": "USppzqoTUHl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 filter_noun_only"
      ],
      "metadata": {
        "id": "xzVDToCpp8ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter_noun_only(earning_transcripts)\n",
        "\n",
        "This function will remove words that were not Nouns from each sentence\n",
        "\n",
        "**Parameter** (earning_transcripts): output from 2.2\n",
        "\n",
        "**Return** (earning_transcripts_noun): a list of lists of words that only contain Nouns"
      ],
      "metadata": {
        "id": "GA7p0xCAqB8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_noun_only(earning_transcripts):\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  earning_transcripts_noun = []\n",
        "  for earning_transcript in earning_transcripts:\n",
        "    text=' '.join(earning_transcript).lower()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tags = nltk.pos_tag(tokens) # use nltk package's function to identify POS\n",
        "    new_transcript=[word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')] # only keep words that are Nouns\n",
        "    earning_transcripts_noun.append(new_transcript)\n",
        "  return earning_transcripts_noun"
      ],
      "metadata": {
        "id": "ksQZFXRzUHuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 stopword_removal"
      ],
      "metadata": {
        "id": "gV6gOHGurAE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stopword_removal(earning_transcripts)\n",
        "\n",
        "This function will remove the stop words from each tokenized sentence\n",
        "\n",
        "**Parameter** (earning_transcripts): output from 2.3\n",
        "\n",
        "**Return** (earning_transcripts_stopword_removal): tokenized sentences with stop words removed"
      ],
      "metadata": {
        "id": "SXQEX6Z9rZcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stopword_removal(earning_transcripts):\n",
        "  stopwords = nltk.corpus.stopwords.words('english') # a list of stop words from nltk package\n",
        "  newStopWords = [\"right\",\"everyone\",\"hey\",\"sorry\",\"joining_us\",\"thing\",\"term\",\"lot\",\"number\",\"time\",\"way\",\"little_bit\",\"yes\",\"something\",\"anything\",\"couple\",\"one\",\"color\",\"prepared_remarks\",\"my_remarks\",\"unidentified_analyst\",\"talk\",\"congrats\",\"sure\",\"call\",\"question\",\"sort\",\"okay\",\"guess\",\"thought\",\"answer\",\"commentary\",\"please\",\"make_sense\",\"wondering_if\",\"hello\",\"comment\",\"guy\",\"hi\",\"bunch\",\"talk_about\",\"yeah\",\"think\",\"taking_my_question\",\"let_me\",\"kind\",\"sense\",\"versus\",\"hi_good\",\"hey_good\",\"quick_follow_up\",\"appreciate\",\"ask\",\"call\",\"you\"] # extra stop words that are manually added\n",
        "  stopwords.extend(newStopWords)\n",
        "  earning_transcripts_stopword_removal=[]\n",
        "  for earning_transcript in earning_transcripts:\n",
        "    new_transcript=[word for word in earning_transcript if word.lower() not in stopwords]\n",
        "    earning_transcripts_stopword_removal.append(new_transcript) # stop words are removed\n",
        "  return earning_transcripts_stopword_removal"
      ],
      "metadata": {
        "id": "jdTuB2hVUHxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 lemmatization"
      ],
      "metadata": {
        "id": "eS94ix_PsRmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatization(earning_transcripts)\n",
        "\n",
        "This function will group together the inflected forms of a word so they can be analysed as a single item\n",
        "\n",
        "**Parameter** (earning_transcripts): output from 2.4\n",
        "\n",
        "**Return** (earning_transcripts_lemmatization): tokenized sentences after lemmatization"
      ],
      "metadata": {
        "id": "u1UOmCPasofN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(earning_transcripts):\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  lemm = WordNetLemmatizer() # use lemmatizer from nltk package\n",
        "  earning_transcripts_lemmatization=[]\n",
        "  for earning_transcript in earning_transcripts:\n",
        "    new_transcript=[lemm.lemmatize(word) for word in earning_transcript]\n",
        "    new_transcript=[word for word in new_transcript if word.lower() not in stopwords]\n",
        "    earning_transcripts_lemmatization.append(new_transcript) # generate a new list that contains tokenized sentences after lemmatization\n",
        "  return earning_transcripts_lemmatization\n"
      ],
      "metadata": {
        "id": "Ey6b-akcUH0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Main Function**"
      ],
      "metadata": {
        "id": "7zPwCiijtUv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Preprocessing\n",
        "\n",
        "This is the main function for preprocessing. Call the function to start preprocessing for a single csv file\n",
        "\n",
        "**Parameter** (input_file_name): the name of the csv file that contains earning transcripts\n",
        "\n",
        "**Parameter** (output_file_name): the name of the csv file that will contain the earning transcripts after preprocessing"
      ],
      "metadata": {
        "id": "-unkFZUNtqDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing( input_file_name,output_file_name):\n",
        "  earning_transcripts = load_data(input_file_name)\n",
        "  print(\"step 1: data loading is completed\")\n",
        "  earning_transcripts_name_removed = split_and_entity_removal(earning_transcripts)\n",
        "  print(\"step 2: unrelated entities is removed\")\n",
        "  earning_transcripts_single_sentence = split_and_expand_contraction(earning_transcripts_name_removed)\n",
        "  print(\"step 3: earning transcripts are splitted into single sentences\")\n",
        "  tokenized_earning_transcripts = lower_case_and_tokenize(earning_transcripts_single_sentence)\n",
        "  print(\"step 4: earning transcripts are tokenized\")\n",
        "  earning_transcripts_bigram_trigram = bigram_trigram(tokenized_earning_transcripts)\n",
        "  print(\"step 5: bigram and trigram formations were completed\")\n",
        "  earning_transcripts_noun = filter_noun_only(earning_transcripts_bigram_trigram)\n",
        "  print(\"step 6: only nouns were kept in earning transcripts\")\n",
        "  earning_transcripts_stopword_removal = stopword_removal(earning_transcripts_noun)\n",
        "  print(\"step 7: stop words were removed from earning transcripts\")\n",
        "  earning_transcripts_lemmatization = lemmatization(earning_transcripts_stopword_removal)\n",
        "  print(\"step 8:lemmatization were completed\")\n",
        "  post_processing_earning_transcript=[]\n",
        "  for earning_transcript in earning_transcripts_lemmatization:\n",
        "    if len(earning_transcript)>1:\n",
        "      post_processing_earning_transcript.append(' '.join(earning_transcript))\n",
        "  df=pd.DataFrame(post_processing_earning_transcript)\n",
        "  df.to_csv(output_file_name, index=False)"
      ],
      "metadata": {
        "id": "W2VBSNBKUH2n"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}